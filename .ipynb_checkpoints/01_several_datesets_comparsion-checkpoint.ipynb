{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. 定义输出目录\n",
    "output_dir = r\"D:\\lakemapping\\9_analysis\\SWOT_lake_distribution\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 创建目录，如果目录已存在则不会报错\n",
    "\n",
    "for layer in layers:\n",
    "    print(f\"正在处理图层: {layer}\")\n",
    "\n",
    "    # 4. 读取当前图层的数据\n",
    "    lakes_gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "\n",
    "    # 6. 按面积区间分类并计算数量及面积\n",
    "    bins = [0.01, 0.1, 1, 10, 100,1000,10000,100000,1000000]\n",
    "    labels = ['0.01-0.1', '0.1-1', '1-10', '10-100','100-1000','1000-10000','10000-100000','100000-1000000']\n",
    "    lakes_gdf['area_category'] = pd.cut(lakes_gdf['ref_area'], bins=bins, labels=labels)\n",
    "\n",
    "    area_summary = lakes_gdf.groupby('area_category').agg(\n",
    "        lake_count=('geometry', 'count'),\n",
    "        total_area=('ref_area', 'sum')\n",
    "    ).reset_index()\n",
    "    # 8. 将每个图层的汇总结果保存为CSV文件\n",
    "    area_summary.to_csv(os.path.join(output_dir, f'{layer}_Summary.csv'), index=False)\n",
    "\n",
    "print(f\"所有图层数据处理完成，并已保存到{output_dir}目录中的CSV文件中\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "continent_list=['af','eu','si','as','au','sa','na','ar','gr']\n",
    "output_dir=r'D:\\lakemapping\\9_analysis\\AWOT_lake_distribution'\n",
    "combined_data = pd.DataFrame(columns=['area_category', 'lake_count', 'total_area'])\n",
    "for continent in continent_list:\n",
    "    # 3. 遍历目录中的所有文件\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith(\".csv\") and  filename.startswith(f'PLD_{continent}'):\n",
    "            # 读取每个CSV文件\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            print(file_path)\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # 将数据添加到合并数据框中\n",
    "            combined_data = pd.concat([combined_data, data])\n",
    "\n",
    "    final_summary = combined_data.groupby('area_category').agg(\n",
    "        total_lake_count=('lake_count', 'sum'),\n",
    "        total_area_sum=('total_area', 'sum')\n",
    "    ).reset_index()\n",
    "    final_summary.to_csv(os.path.join(output_dir, f'PLD_{continent}_lake_area_summary.csv'), index=False)\n",
    "    print(\"done\")\n",
    "\n",
    "print(f\"所有湖泊尺寸区间数据合并完成，并已保存为{os.path.join(output_dir, f'PLD_{continent}_lake_area_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=r'J:\\lakemapping\\postprocess\\v13_241021'\n",
    "gdb=\"5_polygon_afm_mwBL_BigGLAKES.gdb\"#'6_polygon_afm_mergeWith_BigPLD.gdb'#'4_polygon_afm_mergeWith_BigLake.gdb'#\n",
    "gdb_path=os.path.join(base_dir,gdb)\n",
    "output_dir=os.path.join(base_dir,\"stastics_excel\")\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "os.makedirs(output_dir, exist_ok=True)  # 创建目录，如果目录已存在则不会报错\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers[-1:]:\n",
    "    if layer.endswith('BG') and layer.startswith('c'):\n",
    "        print(f\"正在处理图层: {layer}\")\n",
    "\n",
    "        # 4. 读取当前图层的数据\n",
    "        lakes_gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "\n",
    "        # 6. 按面积区间分类并计算数量及面积\n",
    "        bins = [0.005,0.01, 0.1, 1, 10, 100,1000,10000,100000,1000000]\n",
    "        labels = ['0.005-0.01','0.01-0.1', '0.1-1', '1-10.0', '10-100','100-1000','1000-10000','10000-100000','100000-1000000']\n",
    "        lakes_gdf['area_category'] = pd.cut(lakes_gdf['lake_area'], bins=bins, labels=labels)\n",
    "\n",
    "        area_summary = lakes_gdf.groupby('area_category').agg(\n",
    "            lake_count=('geometry', 'count'),\n",
    "            total_area=('lake_area', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        # 8. 将每个图层的汇总结果保存为CSV文件\n",
    "        area_summary.to_csv(os.path.join(output_dir, f'{layer}_Summary.csv'), index=False)\n",
    "\n",
    "print(f\"所有图层数据处理完成，并已保存到{output_dir}目录中的CSV文件中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_summary.to_csv(os.path.join(output_dir, f'{layer}_Summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_gdf = gpd.read_file(r'D:\\lakemapping\\0_auxiliary_data\\GLAKES\\GLAKES.gdb',layer='GLAKES')\n",
    "bins = [0.03,0.1, 1, 10, 100,1000,10000,100000,1000000]\n",
    "labels = ['0.03-0.01','0.1-1', '1-10', '10-100','100-1000','1000-10000','10000-100000','100000-1000000']\n",
    "lakes_gdf['area_category'] = pd.cut(lakes_gdf['Area_bound'], bins=bins, labels=labels)\n",
    "\n",
    "area_summary = lakes_gdf.groupby('area_category').agg(\n",
    "    lake_count=('geometry', 'count'),\n",
    "    total_area=('Area_bound', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# 8. 将每个图层的汇总结果保存为CSV文件\n",
    "area_summary.to_csv(os.path.join(output_dir, f'GLAKES_Summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_gdf = gpd.read_file(r'D:\\lakemapping\\0_auxiliary_data\\HydroLAKES_polys_v10.gdb',layer='HydroLAKES_polys_v10')\n",
    "bins = [0.1, 1, 10, 100,1000,10000,100000,1000000]\n",
    "labels = ['0.1-1', '1-10', '10-100','100-1000','1000-10000','10000-100000','100000-1000000']\n",
    "lakes_gdf['area_category'] = pd.cut(lakes_gdf['Lake_area'], bins=bins, labels=labels)\n",
    "\n",
    "area_summary = lakes_gdf.groupby('area_category').agg(\n",
    "    lake_count=('geometry', 'count'),\n",
    "    total_area=('Lake_area', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# 8. 将每个图层的汇总结果保存为CSV文件\n",
    "area_summary.to_csv(os.path.join(output_dir, f'HydroLakes_Summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir=r'D:\\lakemapping\\9_analysis\\c_lake_distribution'\n",
    "combined_data = pd.DataFrame(columns=['area_category', 'lake_count', 'total_area'])\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # 读取每个CSV文件\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        print(file_path)\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # 将数据添加到合并数据框中\n",
    "        combined_data = pd.concat([combined_data, data])\n",
    "\n",
    "final_summary = combined_data.groupby('area_category').agg(\n",
    "    total_lake_count=('lake_count', 'sum'),\n",
    "    total_area_sum=('total_area', 'sum')\n",
    ").reset_index()\n",
    "final_summary.to_csv(os.path.join(output_dir, f'c_lake_area_summary.csv'), index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. 使用 fiona 获取 Geodatabase 中的图层列表\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "output_dir = r\"D:\\lakemapping\\9_analysis\\my_result\"\n",
    "os.makedirs(output_dir, exist_ok=True) \n",
    "# 3. 定义你想要保留的列，例如 'ref_area' 和 'lake_name'\n",
    "desired_columns = ['sources','lake_id','lake_area']  # 根据你的实际列名进行调整\n",
    "\n",
    "# 4. 定义输出目录\n",
    "output_dir = \"filtered_layers_csv\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 创建目录，如果目录已存在则不会报错\n",
    "\n",
    "for layer in layers:\n",
    "    print(f\"正在处理图层: {layer}\")\n",
    "    \n",
    "    # 5. 读取当前图层的数据\n",
    "    gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "    \n",
    "    # 6. 筛选出你需要的列\n",
    "    filtered_gdf = gdf[desired_columns]\n",
    "    \n",
    "    # 7. 将数据保存为 CSV 文件\n",
    "    output_file = os.path.join(output_dir, f\"{layer}.csv\")\n",
    "    filtered_gdf.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"图层 {layer} 已保存为 {output_file}\")\n",
    "\n",
    "print(f\"所有图层数据处理完成，并已保存到 {output_dir} 目录中的 CSV 文件中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdb_path=r'J:\\lakemapping\\postprocess\\v13_241021\\5_polygon_afm_mwBL_BigGLAKES_point.gdb'\n",
    "lyr='global_polygon_afm_mwBL_BG_point_lt1ha'\n",
    "gdf = gpd.read_file(gdb_path, layer=lyr)\n",
    "# gdf\n",
    "# output_file = os.path.join('J:\\lakemapping\\postprocess\\v13_241021', f\"{lyr}.csv\")\n",
    "# desired_columns=['OBJECTID','sources','lake_area','lake_id']\n",
    "# filtered_gdf = gdf[desired_columns]\n",
    "# filtered_gdf.to_csv(output_file, index=False)\n",
    "# print(f\"图层保存为 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.sort_values(by='lake_area', ascending=False)\n",
    "gdf['lake_id'] = range(7917008, len(gdf) + 7917008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(r'J:\\lakemapping\\postprocess\\v13_241021', f'{lyr}.shp')\n",
    "gdf.to_file(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_38",
   "language": "python",
   "name": "tf_gpu_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
